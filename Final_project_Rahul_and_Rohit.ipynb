{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title:Text Classification Algorithms: A Survey\n",
    "### Focused on: Random Forest Classifier\n",
    "\n",
    "#### Group Member Names :\n",
    "#### 1)Rahul Kasturi - 200629568\n",
    "#### 2)Rohit Sai Kiran Ravula - 200625534\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION:\n",
    "In text classification, one big challenge is that the data is usually unstructured, has too many features (like different words), and most of them don't appear often. This makes it hard for regular models to work well, especially when there are complicated links between words and categories.\n",
    "\n",
    "To solve this, Random Forest (RF) models are often used. They are a type of machine learning method that builds many decision trees and makes predictions based on the majority vote of those trees. RF models are known to be accurate, can handle noise in data, and work well even when there are a lot of features—like in text data.\n",
    "\n",
    "In simple terms, RF models help in sorting or tagging things like documents, emails, or messages by learning patterns from the words in the examples they are trained on.\n",
    "*********************************************************************************************************************\n",
    "### AIM :\n",
    "The main goal of this research paper, focusing on the Random Forest (RF) classifier, is to:\n",
    "\n",
    "->Explore how Random Forest is used for classifying text and documents.\n",
    "\n",
    "->Show how RF fits into natural language processing (NLP) workflows, especially after turning text into useful features and reducing the number of features.\n",
    "\n",
    "->Talk about the strengths of RF, like its ability to handle complex patterns, avoid overfitting, and show which features are important—and also point out its downsides, like being slower and harder to explain than simpler models.\n",
    "\n",
    "->Share real-life examples where RF has been used successfully for organizing or tagging text data.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "https://github.com/kk7nc/Text_Classification.git\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "This research paper gives a detailed overview of the different methods used to classify text. It explains each step in the process, including:\n",
    "\n",
    "->Cleaning and preparing the text <br>\n",
    "->Turning text into numerical features <br>\n",
    "->Reducing the number of features to make models faster and better <br>\n",
    "->Using different models to classify the text <br>\n",
    "->Checking how well the models work <br>\n",
    "The paper looks at many types of models, such as Naïve Bayes, Logistic Regression, SVM, k-NN, and tree-based models like Random Forest. It explains where each model works well and where it doesn't. It also talks about newer approaches like deep learning and word embedding techniques such as Word2Vec, GloVe, and FastText.\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT :\n",
    "With the huge growth of text data from sources like emails, social media, and articles, it's become very important to sort and label this text correctly—for example, to detect spam, understand opinions, or organize medical records. But this is not easy because text data is messy, has many unique words, and often includes complex patterns.\n",
    "\n",
    "The main challenges are:\n",
    "\n",
    "->Finding good ways to turn raw text into useful features <br>\n",
    "->Picking the right models, like Random Forest, to do the classification <br>\n",
    "->Handling problems like too many features, uneven category sizes, and the time it takes to process the data\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "Text classification is used in many areas, such as:\n",
    "\n",
    "->Healthcare (like sorting medical records) <br>\n",
    "->Cybersecurity (like spotting phishing messages) <br>\n",
    "->Social media (like finding harmful or hateful content) <br>\n",
    "->Search engines and recommendation systems <br>\n",
    "Older models like Naïve Bayes and Logistic Regression often struggle when there are too many features or when the data has complex patterns. That’s why models like Random Forest, which use multiple decision trees, are becoming more popular—they can manage complicated data better and give more accurate results.\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "The paper shows how Random Forest (RF), a tree-based model, helps in sorting and classifying documents. Here's how it works:\n",
    "\n",
    "Random Forest builds many decision trees using different parts of the data and random sets of features. This mix of trees helps improve accuracy and avoids overfitting, which is when a model works well on training data but not on new data.\n",
    "\n",
    "Why it’s useful for text classification:\n",
    "\n",
    "->RF can handle text data with many features. <br>\n",
    "->It doesn’t get easily confused by noisy or unimportant words. <br>\n",
    "->It supports tasks with many categories. <br>\n",
    "->It works well even with simple text processing, so it's a good starting point. <br>\n",
    "\n",
    "Some drawbacks:\n",
    "\n",
    "->It’s harder to understand and explain than simpler models. <br>\n",
    "->Training takes more time, especially with very large datasets. <br>\n",
    "->For tasks needing deep understanding of meaning (like context or emotion), deep learning models might do better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "### Reference of Paper selected\n",
    "Research Paper: https://www.mdpi.com/2078-2489/10/4/150?source=post_page--------------------------- <br>\n",
    "Dataset: from sklearn.datasets import fetch_20newsgroups <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html <br>\n",
    "GitHub link: https://github.com/kk7nc/Text_Classification\n",
    "\n",
    "### Explanation\n",
    "I read a research paper called \"Text Classification Algorithms: A Survey\", which explains different ways to classify text. I found it very useful and wanted to see how these methods work in practice.\n",
    "\n",
    "I looked it up on Papers with Code and found its GitHub repository, which includes models like CNN, DNN, RNN, and CRF. I chose to focus on the Random Forest Classifier because it's a traditional model that is easy to understand and works well with many types of data.\n",
    "\n",
    "The GitHub author used the fetch_20newsgroups dataset from sklearn.datasets. This is a popular dataset with 20 different types of news articles, often used to test how well models can sort text into categories. It's a good fit for checking how Random Forest performs with text that has lots of different features.\n",
    "\n",
    "In this setup, standard techniques like TF-IDF were used to turn text into numbers, then the Random Forest model was trained and tested to see how well it could classify the documents.\n",
    "\n",
    "### Dataset\n",
    "fetch_20newsgroups dataset from sklearn.datasets\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "\n",
    "### To find Random_Forest.py file:\n",
    "Text Classification --> code --> Random_Forest.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "#### 1. Importing Libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#### 2. Load the Data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "#### 3. Text Vectorization (TF-IDF)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "#### 4. Train Random Forest Classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, newsgroups_train.target)\n",
    "\n",
    "#### 5. Make Predictions\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "#### 6. Evaluate Performance\n",
    "print(classification_report(newsgroups_test.target, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(newsgroups_test.target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "Overall Accuracy (~77–84%) <br>\n",
    "which means Random Forest can effectively handle sparse text data transformed via TF-IDF.\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "The Random_Forest.py script proves that with just TF-IDF + Random Forest, you can achieve strong baseline performance on a multi-class NLP task like 20 Newsgroups. It’s fast, interpretable, and surprisingly accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimentation done by us\n",
    "We applied the Random Forest Classifier on a new dataset to evaluate the model’s accuracy and see how well it performs on unseen data.\n",
    "\n",
    "### Dataset \n",
    "Email Spam Text Classification Dataset, from Kaggle <br>\n",
    "Link: https://www.kaggle.com/datasets/tapakah68/email-spam-classification\n",
    "\n",
    "### GitHub\n",
    "Please the check the code from below repositorie <br>\n",
    "file name: spam_classifier.py\n",
    "GitHub link: https://github.com/RahulKasturi/email-spam-classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "This project demonstrates the effectiveness of the Random Forest classifier in detecting spam emails using TF-IDF vectorized features. By combining subject and body text, and applying class balancing through SMOTE, we achieved reasonable accuracy and interpretability without deep learning. The model performs well in identifying legitimate emails, but struggles slightly with rare or ambiguous spam messages — a common challenge in real-world datasets.\n",
    "\n",
    "#### Learnings :\n",
    "->Learned how to preprocess real-world textual data (title + content). <br>\n",
    "->Understood how TF-IDF helps convert raw text into meaningful numerical features. <br>\n",
    "->Gained experience with Random Forest, a powerful and interpretable ensemble classifier. <br>\n",
    "->Applied SMOTE to deal with class imbalance — an essential step in email spam filtering. <br>\n",
    "->Practiced creating a fully functional and reproducible ML pipeline. <br>\n",
    "->Learned to document, organize, and publish a public GitHub repository.\n",
    "\n",
    "#### Results Discussion :\n",
    "->Accuracy reached around 75% initially and improved after applying SMOTE and combining input features. <br>\n",
    "->\"Not spam\" emails were detected more accurately, with higher recall and precision.<br>\n",
    "->\"Spam\" emails were harder to classify, likely due to text diversity and smaller class size.<br>\n",
    "->Overall, the model offers a strong baseline but can be enhanced for production use.\n",
    "\n",
    "#### Limitations :\n",
    "->Class imbalance: Despite using SMOTE, the dataset may not fully reflect the diversity of real-world spam. <br>\n",
    "->TF-IDF ignores word order and context — it’s good for frequency, not meaning. <br>\n",
    "->Random Forest lacks semantic understanding: It doesn’t capture nuanced meanings like newer models  <br>\n",
    "->Interpretability is limited to feature importances — can't visualize decision-making as clearly as logistic regression.\n",
    "\n",
    "#### Future Extension :\n",
    "->Try alternative models: Compare with SVM, Logistic Regression, or XGBoost. <br>\n",
    "->Use word embeddings (like GloVe or Word2Vec) for better text understanding. <br>\n",
    "->Integrate deep learning (e.g., LSTM or BERT) for sequence and context modeling. <br>\n",
    "->Add explainability tools like SHAP to interpret model decisions. <br>\n",
    "->Expand dataset with more spam types or emails from different sources. <br>\n",
    "->Deploy as a web app or API to allow live email classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "Research Paper: https://www.mdpi.com/2078-2489/10/4/150?source=post_page--------------------------- <br>\n",
    "Paper GitHub link: https://github.com/kk7nc/Text_Classification <br>\n",
    "Paper Dataset: fetch_20newsgroups dataset from sklearn.datasets\n",
    "\n",
    "New Dataset: https://www.kaggle.com/datasets/tapakah68/email-spam-classification <br>\n",
    "Our GitHub link: https://github.com/RahulKasturi/email-spam-classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
